{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998c57f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from sacremoses import MosesDetokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB#naive bayes model\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier#random forest model \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer#LSTM model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd99855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(address):\n",
    "    data=open(address,'r',encoding='utf-8').readlines()\n",
    "    ID=[]\n",
    "    Type=[]\n",
    "    Content=[]\n",
    "    for row in data:\n",
    "        ID.append(row[0:18])\n",
    "        if not (row[19:26]==\"neutral\"):\n",
    "            Type.append(row[19:27])\n",
    "        else:\n",
    "            Type.append(row[19:26])\n",
    "        Content.append(row[28:])\n",
    "    df_list={\"ID\":ID,\"Type\":Type,\"Content\":Content}\n",
    "    twitter=pd.DataFrame(df_list,columns=['ID','Type','Content','tidy_Content'])\n",
    "    return twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca5a0947",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt,pattern):\n",
    "    r=re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt=re.sub(i, '', input_txt)\n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fc8179e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(twitter_df,language):\n",
    "    twitter_df['tidy_Content']=np.vectorize(remove_pattern)(twitter_df['Content'], \"@[\\w]*\")\n",
    "    twitter_df['tidy_Content']=np.vectorize(remove_pattern)(twitter_df['tidy_Content'], r\"#(\\w+)\")\n",
    "    twitter_df['tidy_Content']=np.vectorize(remove_pattern)(twitter_df['tidy_Content'], r'http://[a-zA-Z0-9.?/&=:]*')\n",
    "    if language=='english':\n",
    "        twitter_df['tidy_Content']=twitter_df['tidy_Content'].str.replace(\"[^a-zA-Z#]\", \" \",regex=True)\n",
    "    twitter_df['tidy_Content']=twitter_df['tidy_Content'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "    token_tweet=twitter_df['tidy_Content'].apply(lambda x: x.split())\n",
    "    stemmer=PorterStemmer()\n",
    "    token_tweet=token_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "\n",
    "    detokenizer=MosesDetokenizer()\n",
    "    for i in range(len(token_tweet)):\n",
    "        token_tweet[i]=detokenizer.detokenize(token_tweet[i], return_str=True)\n",
    "    twitter_df['tidy_Content']=token_tweet\n",
    "    #########################################################################################################\n",
    "    twitter_df['tidy_Content']=twitter_df['tidy_Content'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "    #########################################################################################################\n",
    "    return twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4e2f219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(twitter_df,type):\n",
    "    all_words=\" \".join([text for text in twitter_df['tidy_Content'][twitter_df['Type']==type]])\n",
    "    wordcloud=WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f23170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(df):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in df:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3573286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(HT):\n",
    "    a = nltk.FreqDist(HT)\n",
    "    d = pd.DataFrame({'Hashtag': list(a.keys()),'Count': list(a.values())})\n",
    "    # selecting top 10 most frequent hashtags     \n",
    "    d = d.nlargest(columns=\"Count\", n = 10) \n",
    "    plt.figure(figsize=(16,5))\n",
    "    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "575f364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_y(twitter_df,language):\n",
    "    if language=='english':\n",
    "        tfidf_vectorizer=TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "    elif language=='arabic':\n",
    "        tfidf_vectorizer=TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000)\n",
    "    # TF-IDF feature matrix\n",
    "    tfidf=tfidf_vectorizer.fit_transform(twitter_df['tidy_Content'])\n",
    "    y_validation=pd.DataFrame(columns = ['Type'])\n",
    "    y_validation['Type']=twitter_df['Type'].copy()\n",
    "\n",
    "    #1:positive   0:negative   2:neutral\n",
    "    for i in range(0,y_validation['Type'].size):\n",
    "        if y_validation['Type'][i]=='positive':\n",
    "            y_validation['Type'][i]=1\n",
    "        elif y_validation['Type'][i]=='negative':\n",
    "            y_validation['Type'][i]=0\n",
    "        elif y_validation['Type'][i]=='neutral':\n",
    "            y_validation['Type'][i]=2\n",
    "    return y_validation,tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ec9507d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(y_validation,tfidf):\n",
    "    x_train_valid, x_test_valid, y_train_valid, y_test_valid=train_test_split(tfidf[:4800,:],y_validation['Type'][:4800].values.astype('int'), random_state=0, test_size=0.25)\n",
    "    x_train, x_test,y_train,y_test=train_test_split(tfidf, y_validation['Type'].astype('int'), random_state=0, test_size=0.2)\n",
    "\n",
    "    nb_valid=MultinomialNB()\n",
    "    nb_valid.fit(x_train_valid,y_train_valid)\n",
    "    print('naive bayes validation accuracy:')\n",
    "    print(accuracy_score(y_test_valid,nb_valid.predict(x_test_valid)))\n",
    "\n",
    "    nb_test=MultinomialNB()\n",
    "    nb_test.fit(x_train,y_train)\n",
    "    print('naive bayes test accuracy:')\n",
    "    print(accuracy_score(y_test,nb_test.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89c41e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest(y_validation,tfidf):\n",
    "    x_train_valid, x_test_valid, y_train_valid, y_test_valid=train_test_split(tfidf[:4800,:], y_validation['Type'][:4800].values.astype('int'), random_state=0, test_size=0.25)\n",
    "    x_train, x_test,y_train,y_test=train_test_split(tfidf, y_validation['Type'].astype('int'), random_state=0, test_size=0.2)\n",
    "\n",
    "    rf_valid=RandomForestClassifier(n_estimators=500)\n",
    "    rf_valid.fit(x_train_valid,y_train_valid)\n",
    "    print('random forest valid accuracy:')\n",
    "    print(accuracy_score(y_test_valid,rf_valid.predict(x_test_valid)))\n",
    "\n",
    "    rf_test=RandomForestClassifier(n_estimators=500)\n",
    "    rf_test.fit(x_train,y_train)\n",
    "    print('random forest test accuracy:')\n",
    "    print(accuracy_score(y_test,rf_test.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6cdfbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text_list): \n",
    "    stopwords_rem=False\n",
    "    stopwords_en=stopwords.words('english')\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    tokenizer=TweetTokenizer()\n",
    "    reconstructed_list=[]\n",
    "    for each_text in text_list: \n",
    "        lemmatized_tokens=[]\n",
    "        tokens=tokenizer.tokenize(each_text.lower())\n",
    "        pos_tags=pos_tag(tokens)\n",
    "        for each_token, tag in pos_tags: \n",
    "            if tag.startswith('NN'): \n",
    "                pos='n'\n",
    "            elif tag.startswith('VB'): \n",
    "                pos='v'\n",
    "            elif tag.startswith('JJ'): \n",
    "                pos='a'\n",
    "            elif tag.startswith('R'):\n",
    "                pos='r'\n",
    "            lemmatized_token=lemmatizer.lemmatize(each_token, pos)\n",
    "            if stopwords_rem: # False \n",
    "                if lemmatized_token not in stopwords_en: \n",
    "                    lemmatized_tokens.append(lemmatized_token)\n",
    "            else: \n",
    "                lemmatized_tokens.append(lemmatized_token)\n",
    "        reconstructed_list.append(' '.join(lemmatized_tokens))\n",
    "    return reconstructed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80378416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_model(twitter_df,y_validation,language):\n",
    "    # 将数据分解为训练集和测试集\n",
    "    if language=='english':\n",
    "        lstm_df=pd.DataFrame(columns=['tidy_Content'])\n",
    "        lstm_df['tidy_Content']=np.vectorize(remove_pattern)(twitter_df['Content'], \"@[\\w]*\")\n",
    "        lstm_df['tidy_Content']=np.vectorize(remove_pattern)(lstm_df['tidy_Content'], r\"#(\\w+)\")\n",
    "        lstm_df['tidy_Content']=np.vectorize(remove_pattern)(lstm_df['tidy_Content'], r'http://[a-zA-Z0-9.?/&=:]*')\n",
    "        lstm_df['tidy_Content']=lstm_df['tidy_Content'].str.replace(\"[^a-zA-Z#]\", \" \",regex=True)\n",
    "        lstm_df['tidy_Content']=lstm_df['tidy_Content'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "        X_train, X_test,y_train,y_test=train_test_split(lstm_df['tidy_Content'], y_validation['Type'], random_state=0, test_size=0.2)\n",
    "    elif language=='arabic':\n",
    "        X_train, X_test,y_train,y_test=train_test_split(twitter_df['Content'], y_validation['Type'], random_state=0, test_size=0.2)\n",
    "\n",
    "    # 拟合并转换数据\n",
    "    X_train=data_cleaning(X_train)\n",
    "    X_test=data_cleaning(X_test)\n",
    "    tokenizer=Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    vocab_size=len(tokenizer.word_index)+1\n",
    "    print(f'Vocab Size: {vocab_size}')\n",
    "    X_train=pad_sequences(tokenizer.texts_to_sequences(X_train), maxlen=40)\n",
    "    X_test=pad_sequences(tokenizer.texts_to_sequences(X_test), maxlen=40)\n",
    "    y_train=to_categorical(y_train)\n",
    "    y_test=to_categorical(y_test)\n",
    "\n",
    "    # 创建带有嵌入层的LSTM模型并拟合训练数据\n",
    "    model=Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size,output_dim=100,input_length=40))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(128)))\n",
    "    model.add(layers.Dense(3,activation='softmax'))\n",
    "    model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    model.fit(X_train,y_train,batch_size=256,epochs=5,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "42cfb2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes validation accuracy:\n",
      "0.6041666666666666\n",
      "naive bayes test accuracy:\n",
      "0.5991666666666666\n",
      "random forest valid accuracy:\n",
      "0.6208333333333333\n",
      "random forest test accuracy:\n",
      "0.6091666666666666\n",
      "Vocab Size: 7980\n",
      "Epoch 1/5\n",
      "19/19 [==============================] - 7s 385ms/step - loss: 1.0083 - accuracy: 0.4931 - val_loss: 0.9698 - val_accuracy: 0.5200\n",
      "Epoch 2/5\n",
      "19/19 [==============================] - 7s 392ms/step - loss: 0.9283 - accuracy: 0.5404 - val_loss: 0.9015 - val_accuracy: 0.5833\n",
      "Epoch 3/5\n",
      "19/19 [==============================] - 8s 396ms/step - loss: 0.7553 - accuracy: 0.6702 - val_loss: 0.8845 - val_accuracy: 0.5933\n",
      "Epoch 4/5\n",
      "19/19 [==============================] - 7s 367ms/step - loss: 0.5793 - accuracy: 0.7590 - val_loss: 0.9554 - val_accuracy: 0.6017\n",
      "Epoch 5/5\n",
      "19/19 [==============================] - 7s 374ms/step - loss: 0.3997 - accuracy: 0.8413 - val_loss: 1.0282 - val_accuracy: 0.6025\n"
     ]
    }
   ],
   "source": [
    "twitter_df=to_csv('../Datasets/twitter-2016train-A.txt')\n",
    "twitter_df=preprocess(twitter_df,'english')\n",
    "\n",
    "#plot_wordcloud(twitter_df,'positive')\n",
    "#plot_wordcloud(twitter_df,'negative')\n",
    "#plot_wordcloud(twitter_df,'neutral')\n",
    "\n",
    "# extracting hashtags from positive tweets\n",
    "HT_positive = hashtag_extract(twitter_df['Content'][twitter_df['Type']=='positive'])\n",
    "# extracting hashtags from negative tweets\n",
    "HT_negative = hashtag_extract(twitter_df['Content'][twitter_df['Type']=='negative'])\n",
    "# extracting hashtags from neutral tweets\n",
    "HT_neutral = hashtag_extract(twitter_df['Content'][twitter_df['Type']=='neutral'])\n",
    "# unnesting list\n",
    "HT_positive=sum(HT_positive,[])\n",
    "HT_negative=sum(HT_negative,[])\n",
    "HT_neutral=sum(HT_neutral,[])\n",
    "#plot_histogram(HT_positive)\n",
    "#plot_histogram(HT_negative)\n",
    "#plot_histogram(HT_neutral)\n",
    "\n",
    "print('FOR ENGLISH:')\n",
    "y_validation,tfidf=preprocess_y(twitter_df,'english')\n",
    "naive_bayes(y_validation,tfidf)\n",
    "random_forest(y_validation,tfidf)\n",
    "LSTM_model(twitter_df,y_validation,'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2232b3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive bayes validation accuracy:\n",
      "0.5983313468414779\n",
      "naive bayes test accuracy:\n",
      "0.5991058122205664\n",
      "random forest valid accuracy:\n",
      "0.5530393325387366\n",
      "random forest test accuracy:\n",
      "0.5827123695976155\n",
      "Vocab Size: 18133\n",
      "Epoch 1/5\n",
      "11/11 [==============================] - 4s 374ms/step - loss: 1.0714 - accuracy: 0.4277 - val_loss: 1.0353 - val_accuracy: 0.4754\n",
      "Epoch 2/5\n",
      "11/11 [==============================] - 4s 360ms/step - loss: 1.0359 - accuracy: 0.4989 - val_loss: 1.0228 - val_accuracy: 0.5082\n",
      "Epoch 3/5\n",
      "11/11 [==============================] - 4s 384ms/step - loss: 0.9676 - accuracy: 0.5831 - val_loss: 0.9785 - val_accuracy: 0.5186\n",
      "Epoch 4/5\n",
      "11/11 [==============================] - 5s 430ms/step - loss: 0.7189 - accuracy: 0.6747 - val_loss: 0.9131 - val_accuracy: 0.5842\n",
      "Epoch 5/5\n",
      "11/11 [==============================] - 5s 450ms/step - loss: 0.4430 - accuracy: 0.8376 - val_loss: 1.0982 - val_accuracy: 0.5618\n"
     ]
    }
   ],
   "source": [
    "twitter_df_a=to_csv('../Datasets/twitter-2016train-A-arabic.txt')\n",
    "twitter_df_a=preprocess(twitter_df_a,'arabic')\n",
    "\n",
    "#plot_wordcloud(twitter_df_a,'positive')\n",
    "#plot_wordcloud(twitter_df_a,'negative')\n",
    "#plot_wordcloud(twitter_df_a,'neutral')\n",
    "\n",
    "# extracting hashtags from positive tweets\n",
    "HT_positive_a = hashtag_extract(twitter_df_a['Content'][twitter_df_a['Type']=='positive'])\n",
    "# extracting hashtags from negative tweets\n",
    "HT_negative_a = hashtag_extract(twitter_df_a['Content'][twitter_df_a['Type']=='negative'])\n",
    "# extracting hashtags from neutral tweets\n",
    "HT_neutral_a = hashtag_extract(twitter_df_a['Content'][twitter_df_a['Type']=='neutral'])\n",
    "# unnesting list\n",
    "HT_positive_a=sum(HT_positive_a,[])\n",
    "HT_negative_a=sum(HT_negative_a,[])\n",
    "HT_neutral_a=sum(HT_neutral_a,[])\n",
    "#plot_histogram(HT_positive_a)\n",
    "#plot_histogram(HT_negative_a)\n",
    "#plot_histogram(HT_neutral_a)\n",
    "\n",
    "print('FOR ARABIC:')\n",
    "y_validation_a,tfidf_a=preprocess_y(twitter_df_a,'arabic')\n",
    "naive_bayes(y_validation_a,tfidf_a)\n",
    "random_forest(y_validation_a,tfidf_a)\n",
    "LSTM_model(twitter_df_a,y_validation_a,'arabic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef621e71",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6015bbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
