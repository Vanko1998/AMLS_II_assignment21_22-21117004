{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da83a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.stem.porter import *\n",
    "import nltk\n",
    "from sacremoses import MosesDetokenizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4489d68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(address):\n",
    "    twitter_df=pd.read_csv(address, sep=\"\\t\",names=['ID','Event','Type','Content'])\n",
    "    return twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "50ee711a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt,pattern):\n",
    "    r=re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt=re.sub(i, '', input_txt)\n",
    "    return input_txt  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "736c43a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(twitter_df,language):\n",
    "    twitter_df['tidy_Content']=np.vectorize(remove_pattern)(twitter_df['Content'], \"@[\\w]*\")\n",
    "    twitter_df['tidy_Content']=np.vectorize(remove_pattern)(twitter_df['tidy_Content'], r\"#(\\w+)\")\n",
    "    twitter_df['tidy_Content']=np.vectorize(remove_pattern)(twitter_df['tidy_Content'], r'http://[a-zA-Z0-9.?/&=:]*')\n",
    "    if language=='english':\n",
    "        twitter_df['tidy_Content']=twitter_df['tidy_Content'].str.replace(\"[^a-zA-Z#]\", \" \",regex=True)\n",
    "    twitter_df['tidy_Content']=twitter_df['tidy_Content'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "\n",
    "    token_tweet=twitter_df['tidy_Content'].apply(lambda x: x.split())\n",
    "    stemmer=PorterStemmer()\n",
    "    token_tweet=token_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "\n",
    "    detokenizer=MosesDetokenizer()\n",
    "    for i in range(len(token_tweet)):\n",
    "        token_tweet[i]=detokenizer.detokenize(token_tweet[i], return_str=True)\n",
    "    twitter_df['tidy_Content']=token_tweet\n",
    "    #########################################################################################################\n",
    "    twitter_df['tidy_Content']=twitter_df['tidy_Content'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "    #########################################################################################################\n",
    "    return twitter_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcc0f9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wordcloud(twitter_df,event):\n",
    "    all_words=\" \".join([text for text in twitter_df['tidy_Content'][twitter_df['Event']==event]])\n",
    "    wordcloud=WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "17ea13f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect hashtags\n",
    "def hashtag_extract(df):\n",
    "    hashtags = []\n",
    "    # Loop over the words in the tweet\n",
    "    for i in df:\n",
    "        ht = re.findall(r\"#(\\w+)\", i)\n",
    "        hashtags.append(ht)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93ba1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(HT):\n",
    "    a = nltk.FreqDist(HT)\n",
    "    d = pd.DataFrame({'Hashtag': list(a.keys()),'Count': list(a.values())})\n",
    "    # selecting top 10 most frequent hashtags     \n",
    "    d = d.nlargest(columns=\"Count\", n = 10) \n",
    "    plt.figure(figsize=(16,5))\n",
    "    ax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\n",
    "    ax.set(ylabel = 'Count')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "33e2d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_y(twitter_df,language):\n",
    "    if language=='english':\n",
    "        tfidf_vectorizer=TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
    "    elif language=='arabic':\n",
    "        tfidf_vectorizer=TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000)\n",
    "    # TF-IDF feature matrix\n",
    "    tfidf=tfidf_vectorizer.fit_transform(twitter_df['tidy_Content'])\n",
    "    y_validation=pd.DataFrame(columns = ['Type'])\n",
    "    y_validation['Type']=twitter_df['Type'].copy()\n",
    "\n",
    "    #1:positive   0:negative   2:neutral\n",
    "    for i in range(0,y_validation['Type'].size):\n",
    "        if y_validation['Type'][i]=='positive':\n",
    "            y_validation['Type'][i]=1\n",
    "        elif y_validation['Type'][i]=='negative':\n",
    "            y_validation['Type'][i]=0\n",
    "    return y_validation,tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7dc3b695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_one_event(event,twitter_df):\n",
    "    plot_wordcloud(twitter_df,event)\n",
    "    # extracting hashtags from positive tweets\n",
    "    HT_event= hashtag_extract(twitter_df['Content'][twitter_df['Event']==event])\n",
    "    # unnesting list\n",
    "    HT_event=sum(HT_event,[])\n",
    "    plot_histogram(HT_event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "02894617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_event(event,twitter_df):\n",
    "    df=pd.DataFrame(columns=['ID','Type','Content','tidy_Content'])\n",
    "    for i in range(0,twitter_df.index.size):\n",
    "        if twitter_df['Event'][i]==event:\n",
    "            df_new=pd.DataFrame(twitter_df.iloc[i])\n",
    "            df_new_T=pd.DataFrame(df_new.values.T, index=df_new.columns, columns=df_new.index)\n",
    "            df=pd.concat([df,df_new_T],axis=0)\n",
    "            df=df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2afaa207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_design(x_train,x_test, y_train,y_test):#for SVM model\n",
    "    classifier = svm.SVC(kernel='linear')#use linear instead of other kernels\n",
    "    classifier.fit(x_train, y_train)#training model\n",
    "    pred = classifier.predict(x_test)#make prediction\n",
    "    print(accuracy_score(y_test, pred))#show results\n",
    "    return accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ec802cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_design(X_train,X_test, y_train,y_test,k):#for Knn model\n",
    "    neigh = KNeighborsClassifier(n_neighbors=k)#classify data\n",
    "    neigh.fit(X_train, y_train) # Fit KNN model\n",
    "    return accuracy_score(y_test,neigh.predict(X_test))#return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "404c292d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_model(twitter_df,language):\n",
    "    y_validation,tfidf=preprocess_y(twitter_df,language)\n",
    "    x_train, x_test,y_train,y_test=train_test_split(tfidf, y_validation['Type'].astype('int'), random_state=42, test_size=0.2)\n",
    "    x_train_valid, x_test_valid, y_train_valid, y_test_valid=train_test_split(tfidf[:(y_validation.size*4)//5],y_validation['Type'].astype('int')[:(y_validation.size*4)//5], random_state=None, test_size=0.25)\n",
    "\n",
    "    acc_valid=svm_design(x_train_valid, x_test_valid, y_train_valid, y_test_valid)\n",
    "    acc_test=svm_design(x_train, x_test,y_train,y_test)\n",
    "    return acc_valid,acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "398d431b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_model(twitter_df,language):\n",
    "    y_validation,tfidf=preprocess_y(twitter_df,language)\n",
    "    x_train, x_test,y_train,y_test=train_test_split(tfidf, y_validation['Type'].astype('int'), random_state=42, test_size=0.2)\n",
    "    x_train_valid, x_test_valid, y_train_valid, y_test_valid=train_test_split(tfidf[:(y_validation.size*4)//5],y_validation['Type'].astype('int')[:(y_validation.size*4)//5], random_state=None, test_size=0.25)\n",
    "\n",
    "    acc_valid=knn_design(x_train_valid, x_test_valid, y_train_valid, y_test_valid,1)\n",
    "    acc_test=knn_design(x_train, x_test,y_train,y_test,1)\n",
    "    return acc_valid,acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7a9d0aa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disneyland\n",
      "1.0\n",
      "1.0\n",
      "eric church\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "fleetwood mac\n",
      "0.9473684210526315\n",
      "1.0\n",
      "bob marley\n",
      "1.0\n",
      "0.9473684210526315\n",
      "magic mike xxl\n",
      "1.0\n",
      "0.9473684210526315\n",
      "ac/dc\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The number of classes has to be greater than one; got 1 class",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-744dcae29fd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mone_event\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtwitter_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0macc_v_s\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc_t_s\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;31m#acc_v_k,acc_t_k=knn_model(df,'english)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m#v_avg_k+=acc_v_k\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-bfbfda57ef11>\u001b[0m in \u001b[0;36msvm_model\u001b[1;34m(twitter_df, language)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mx_train_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_valid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_validation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'int'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_validation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0macc_valid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_design\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0macc_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_design\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0macc_valid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc_test\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-bdd31951a061>\u001b[0m in \u001b[0;36msvm_design\u001b[1;34m(x_train, x_test, y_train, y_test)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msvm_design\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;31m#for SVM model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVC\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'linear'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#use linear instead of other kernels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#training model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#make prediction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#show results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    171\u001b[0m                                        accept_large_sparse=False)\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         sample_weight = np.asarray([]\n",
      "\u001b[1;32mD:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_validate_targets\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    556\u001b[0m                                                   classes=cls, y=y_)\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 558\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    559\u001b[0m                 \u001b[1;34m\"The number of classes has to be greater than one; got %d\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    560\u001b[0m                 \" class\" % len(cls))\n",
      "\u001b[1;31mValueError\u001b[0m: The number of classes has to be greater than one; got 1 class"
     ]
    }
   ],
   "source": [
    "twitter_df=to_csv('../Datasets/twitter-2016train-BD.txt')\n",
    "twitter_df=preprocess(twitter_df,'english')\n",
    "all_event=twitter_df['Event'].value_counts()\n",
    "all_event=all_event.index.values\n",
    "v_avg_k=0\n",
    "t_avg_k=0\n",
    "v_avg_s=0\n",
    "t_avg_s=0\n",
    "for event in all_event:\n",
    "    df=one_event(event,twitter_df)\n",
    "    #print(event)\n",
    "    #naive_bayes(y_validation,tfidf)\n",
    "    #acc_v_s,acc_t_s=svm_model(df,'english')\n",
    "    acc_v_k,acc_t_k=knn_model(df,'english)\n",
    "    v_avg_k+=acc_v_k\n",
    "    t_avg_k+=acc_t_k\n",
    "    #v_avg_s+=acc_v_s\n",
    "    #t_avg_s+=acc_t_s\n",
    "v_avg_k=v_avg_k/60\n",
    "t_avg_k=t_avg_k/60\n",
    "#v_avg_s=v_avg_s/60\n",
    "#t_avg_s=t_avg_s/60\n",
    "print('knn valid average accuracy:')\n",
    "print(v_avg_k)\n",
    "print('knn test accuracy:')\n",
    "print(t_avg_k)\n",
    "#print('svm valid average accuracy:')\n",
    "#print(v_avg_s)\n",
    "#print('svm test accuracy:')\n",
    "#print(t_avg_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a51ede01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn valid average accuracy:\n",
      "0.7601485860352644\n",
      "knn test accuracy:\n",
      "0.7750636786320179\n"
     ]
    }
   ],
   "source": [
    "twitter_df_a=to_csv('../Datasets/twitter-2016train-BD-arabic.txt')\n",
    "twitter_df_a=preprocess(twitter_df_a,'arabic')\n",
    "\n",
    "all_event_a=twitter_df_a['Event'].value_counts()\n",
    "all_event_a=all_event_a.index.values\n",
    "v_avg_k_a=0\n",
    "t_avg_k_a=0\n",
    "v_avg_s_a=0\n",
    "t_avg_s_a=0\n",
    "for event in all_event_a:\n",
    "    df=one_event(event,twitter_df_a)\n",
    "    #print(event)\n",
    "    #acc_v_s,acc_t_s=svm_model(df,'arabic')\n",
    "    acc_v_k_a,acc_t_k_a=knn_model(df,'arabic')\n",
    "    v_avg_k_a+=acc_v_k_a\n",
    "    t_avg_k_a+=acc_t_k_a\n",
    "    #v_avg_s+=acc_v_s\n",
    "    #t_avg_s+=acc_t_s\n",
    "v_avg_k_a=v_avg_k_a/34\n",
    "t_avg_k_a=t_avg_k_a/34\n",
    "#v_avg_s=v_avg_s/60\n",
    "#t_avg_s=t_avg_s/60\n",
    "print('knn valid average accuracy:')\n",
    "print(v_avg_k_a)\n",
    "print('knn test accuracy:')\n",
    "print(t_avg_k_a)\n",
    "#print('Arabic svm valid average accuracy:')\n",
    "#print(v_avg_s)\n",
    "#print('Arabic svm test accuracy:')\n",
    "#print(t_avg_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154d6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
